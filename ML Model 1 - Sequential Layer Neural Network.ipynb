{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Language\n",
    "\n",
    "https://keras.io/guides/sequential_model/\n",
    "\n",
    "> When to use a Sequential model\n",
    "A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "\n",
    "## 2. Model Architecture:\n",
    "1. Embedding layer\n",
    "    - Helps model understand 'meaning' of words by mapping them to representative vector space instead of semantic integers\n",
    "2. Stacked LSTM layers\n",
    "    - Stacked LSTMs add more depth than additional cells in a single LSTM layer (see paper: https://arxiv.org/abs/1303.5778)\n",
    "    - The first LSTM layer must have `return sequences` flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states\n",
    "3. Dense (regression) layer with ReLU activation\n",
    "4. Dense layer with Softmax activation \n",
    "    - Outputs word probability across entire vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Neural Net Preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Neural Net Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Neural Net Training\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of training sentences: ', 58)\n"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "import pandas as pd\n",
    "DATA_DIRECTORY = 'data'\n",
    "TRAIN_DATA_TEXT_GEN_FORMAT_FILE_NAME = 'youtube_video_ids_with_transcript_text_and_author.csv'\n",
    "train_df = pd.read_csv(DATA_DIRECTORY + '/' + TRAIN_DATA_TEXT_GEN_FORMAT_FILE_NAME)\n",
    "# Selecting Dan Lok as author style to emulate\n",
    "author = train_df[train_df['author'] == 'DAN'][\"text\"]\n",
    "print('Number of training sentences: ',author.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vocabulary size in this corpus: ', 7125)\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(author.values)\n",
    "sequences = tokenizer.texts_to_sequences(author.values)\n",
    "\n",
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "print('Vocabulary size in this corpus: ', vocab_size)\n",
    "\n",
    "# Training on 19 words to predict the 20th\n",
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "seq = []\n",
    "# Sliding window to generate train data\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 19, 50)            356300    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 19, 150)           120600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7125)              1075875   \n",
      "=================================================================\n",
      "Total params: 1,756,025\n",
      "Trainable params: 1,756,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    LSTM(150),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CoLab Model Training 1-2+ Hours \n",
    "### *20x Speed Up TF GPU*\n",
    "\n",
    "[colab.research.google.com/drive/1Ckybg1q91bV-tj7ohu_mIeaAn_WZPXxJ#scrollTo=5WBM8bENqPpQ](https://colab.research.google.com/drive/1Ckybg1q91bV-tj7ohu_mIeaAn_WZPXxJ#scrollTo=5WBM8bENqPpQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HD5 NNWeights\n",
    "### Result: Huge Vector of Weights in HD5 File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/model_weights.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/*hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Measure Track Accuracy\n",
    "### Result: Huge Vector of Weights in HD5 File Format\n",
    "#### `loss: 1.3737 - accuracy: 0.6570`\n",
    "#### `100 Epochs` ~1-2+ hrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
